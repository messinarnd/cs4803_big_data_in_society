{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# from aif360.datasets import GermanDataset\n",
    "from aif360.datasets import StructuredDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import LFR\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "# from aif360.metrics import common_utils\n",
    "# from common_utils import compute_metrics\n",
    "\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import StandardDataset\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "from collections import OrderedDict\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def compute_metrics(dataset_true, dataset_pred, \n",
    "                    unprivileged_groups, privileged_groups,\n",
    "                    disp = True):\n",
    "    \"\"\" Compute the key metrics \"\"\"\n",
    "    classified_metric_pred = ClassificationMetric(dataset_true,\n",
    "                                                 dataset_pred, \n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    metrics = OrderedDict()\n",
    "    metrics[\"Balanced accuracy\"] = 0.5*(classified_metric_pred.true_positive_rate()+\n",
    "                                             classified_metric_pred.true_negative_rate())\n",
    "    metrics[\"Statistical parity difference\"] = classified_metric_pred.statistical_parity_difference()\n",
    "    metrics[\"Disparate impact\"] = classified_metric_pred.disparate_impact()\n",
    "    metrics[\"Average odds difference\"] = classified_metric_pred.average_odds_difference()\n",
    "    metrics[\"Equal opportunity difference\"] = classified_metric_pred.equal_opportunity_difference()\n",
    "    metrics[\"Theil index\"] = classified_metric_pred.theil_index()\n",
    "    \n",
    "    if disp:\n",
    "        for k in metrics:\n",
    "            print(\"%s = %.4f\" % (k, metrics[k]))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adultData2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age2</th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>Education</th>\n",
       "      <th>Education-num</th>\n",
       "      <th>Marrital-Status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Capital-Gain</th>\n",
       "      <th>Capital-Loss</th>\n",
       "      <th>Hours-Per-Week</th>\n",
       "      <th>Native-Country</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age2  Age          Workclass  fnlwgt  Education  Education-num  \\\n",
       "0     1   39          State-gov   77516          1             13   \n",
       "1     0   50   Self-emp-not-inc   83311          1             13   \n",
       "2     1   38            Private  215646          0              9   \n",
       "3     0   53            Private  234721          0              7   \n",
       "4     1   28            Private  338409          1             13   \n",
       "\n",
       "       Marrital-Status          Occupation    Relationship    Race  Sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White    0   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White    0   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White    0   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black    0   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black    1   \n",
       "\n",
       "   Capital-Gain  Capital-Loss  Hours-Per-Week  Native-Country  Income  \n",
       "0          2174             0              40   United-States       0  \n",
       "1             0             0              13   United-States       0  \n",
       "2             0             0              40   United-States       0  \n",
       "3             0             0              40   United-States       0  \n",
       "4             0             0              40            Cuba       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protected Class = Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Priveleged Class = Male\n",
    "##### Unprivileged Class = Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_wrapper_income_sex(outcome, protected, unprivileged_groups, privileged_groups,\n",
    "                          favorable_label, unfavorable_label):\n",
    "    \"\"\" A wraper function to create aif360 dataset from outcome and protected in numpy array format.\n",
    "    \"\"\"\n",
    "    df2 = pd.DataFrame(data=outcome,\n",
    "                      columns=['outcome'])\n",
    "    df2['Sex'] = protected\n",
    "    \n",
    "    dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                       unfavorable_label=unfavorable_label,\n",
    "                                       df=df2,\n",
    "                                       label_names=['outcome'],\n",
    "                                       protected_attribute_names=['Sex'],\n",
    "                                       unprivileged_protected_attributes=unprivileged_groups)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "ages_to_consider = [0,1]\n",
    "unprivileged_groups = [{'Sex': 1.0}]\n",
    "privileged_groups = [{'Sex': 0.0}]\n",
    "favorable_label = 0.0 \n",
    "unfavorable_label = 1.0\n",
    "\n",
    "#print(df['Sex'])\n",
    "#print(np.array(df['Sex']))\n",
    "\n",
    "myDataSet = dataset_wrapper_income_sex(outcome = np.array(df['Income']), protected = np.array(df['Sex']), \n",
    "                            unprivileged_groups=unprivileged_groups,\n",
    "                            privileged_groups=privileged_groups,\n",
    "                            favorable_label=favorable_label,\n",
    "                            unfavorable_label=unfavorable_label)  \n",
    "\n",
    "\n",
    "# print(myDataSet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_orig = myDataSet(protected_attribute_names=['Sex'],           # this dataset also contains protected\n",
    "#                                                                           # attribute for \"sex\" which we do not\n",
    "#                                                                           # consider in this evaluation\n",
    "#                              privileged_classes=[lambda x: x == 0],      # age >=25 is considered privileged\n",
    "#                             ) # ignore sex-related attributes\n",
    "\n",
    "dataset_orig = myDataSet\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups2 = [{'Sex': 0}]\n",
    "unprivileged_groups2 = [{'Sex': 1}]\n",
    "\n",
    "# print(dataset_orig_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original Income Sex training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.195396\n",
      "Disparate Impact = 1.281900\n",
      "Base Rate = 0.757590\n",
      "Consistency = 0.757590\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "display(Markdown(\"#### Original Income Sex training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_orig_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_orig_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_orig_train.consistency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Reweighed Income Sex training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.001273\n",
      "Disparate Impact = 0.998330\n",
      "Base Rate = 0.761875\n",
      "Consistency = 0.605432\n"
     ]
    }
   ],
   "source": [
    "## REWEIGHING\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups2,\n",
    "                privileged_groups=privileged_groups2)\n",
    "dataset_transf = RW.fit_transform(dataset_orig)\n",
    "dataset_transf_train, dataset_transf_test = dataset_transf.split([0.7], shuffle=True)\n",
    "\n",
    "\n",
    "metric_RW_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "display(Markdown(\"#### Reweighed Income Sex training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_RW_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_RW_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_RW_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_RW_train.consistency())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### DIR Income Sex training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.192417\n",
      "Disparate Impact = 1.276748\n",
      "Base Rate = 0.758951\n",
      "Consistency = 0.602159\n"
     ]
    }
   ],
   "source": [
    "# Disparate Impact Remover\n",
    "\n",
    "DIR = DisparateImpactRemover(repair_level = 1.0)\n",
    "\n",
    "dataset_transf_DIR = DIR.fit_transform(dataset_orig)\n",
    "dataset_transf_DIR_train, dataset_transf_DIR_test = dataset_transf_DIR.split([0.7], shuffle=True)\n",
    "\n",
    "metric_DIR_train = BinaryLabelDatasetMetric(dataset_transf_DIR_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "\n",
    "# print()\n",
    "\n",
    "display(Markdown(\"#### DIR Income Sex training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_DIR_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_DIR_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_DIR_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_DIR_train.consistency())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protected Class = Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_wrapper_income_age(outcome, protected, unprivileged_groups, privileged_groups,\n",
    "                          favorable_label, unfavorable_label):\n",
    "    \"\"\" A wraper function to create aif360 dataset from outcome and protected in numpy array format.\n",
    "    \"\"\"\n",
    "    df2 = pd.DataFrame(data=outcome,\n",
    "                      columns=['outcome'])\n",
    "    df2['Age2'] = protected\n",
    "    \n",
    "    dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                       unfavorable_label=unfavorable_label,\n",
    "                                       df=df2,\n",
    "                                       label_names=['outcome'],\n",
    "                                       protected_attribute_names=['Age2'],\n",
    "                                       unprivileged_protected_attributes=unprivileged_groups)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "ages_to_consider = [0,1]\n",
    "unprivileged_age_groups = [{'Age2': 0.0}]\n",
    "privileged_age_groups = [{'Age2': 1.0}]\n",
    "favorable_label = 0.0 \n",
    "unfavorable_label = 1.0\n",
    "\n",
    "#print(df['Sex'])\n",
    "#print(np.array(df['Sex']))\n",
    "\n",
    "myAgeDataSet = dataset_wrapper_income_age(outcome = np.array(df['Income']), protected = np.array(df['Age2']), \n",
    "                            unprivileged_groups=unprivileged_age_groups,\n",
    "                            privileged_groups=privileged_age_groups,\n",
    "                            favorable_label=favorable_label,\n",
    "                            unfavorable_label=unfavorable_label)  \n",
    "\n",
    "\n",
    "# print(myAgeDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_age_orig = myAgeDataSet\n",
    "dataset_age_orig_train, dataset_age_orig_test = dataset_age_orig.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_age_groups2 = [{'Age2': 1}]\n",
    "unprivileged_age_groups2 = [{'Age2': 0}]\n",
    "\n",
    "# print(dataset_age_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original Income Age training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.196943\n",
      "Disparate Impact = 0.767144\n",
      "Base Rate = 0.759872\n",
      "Consistency = 0.629958\n"
     ]
    }
   ],
   "source": [
    "metric_age_orig_train = BinaryLabelDatasetMetric(dataset_age_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_age_groups2,\n",
    "                                             privileged_groups=privileged_age_groups2)\n",
    "display(Markdown(\"#### Original Income Age training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_age_orig_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_age_orig_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_age_orig_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_age_orig_train.consistency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Reweighed Income Age training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.001247\n",
      "Disparate Impact = 1.001640\n",
      "Base Rate = 0.761347\n",
      "Consistency = 0.681651\n"
     ]
    }
   ],
   "source": [
    "## REWEIGHING\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_age_groups2,\n",
    "                privileged_groups=privileged_age_groups2)\n",
    "dataset_age_transf_rw = RW.fit_transform(dataset_age_orig)\n",
    "dataset_age_transf_rw_train, dataset_age_transf_rw_test = dataset_age_transf_rw.split([0.7], shuffle=True)\n",
    "\n",
    "\n",
    "metric_age_RW_train = BinaryLabelDatasetMetric(dataset_age_transf_rw_train, \n",
    "                                             unprivileged_groups=unprivileged_age_groups2,\n",
    "                                             privileged_groups=privileged_age_groups2)\n",
    "display(Markdown(\"#### Reweighed Income Age training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_age_RW_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_age_RW_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_age_RW_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_age_RW_train.consistency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### DIR Income Age training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.201067\n",
      "Disparate Impact = 0.763228\n",
      "Base Rate = 0.761451\n",
      "Consistency = 0.605151\n"
     ]
    }
   ],
   "source": [
    "# Disparate Impact Remover\n",
    "\n",
    "DIR = DisparateImpactRemover(repair_level = 0.8)\n",
    "\n",
    "dataset_age_transf_dir = DIR.fit_transform(dataset_age_orig)\n",
    "dataset_age_transf_dir_train, dataset_age_transf_dir_test = dataset_age_transf_dir.split([0.7], shuffle=True)\n",
    "\n",
    "metric_age_dir_train = BinaryLabelDatasetMetric(dataset_age_transf_dir_train, \n",
    "                                             unprivileged_groups=unprivileged_age_groups2,\n",
    "                                             privileged_groups=privileged_age_groups2)\n",
    "\n",
    "display(Markdown(\"#### DIR Income Age training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_age_dir_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_age_dir_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_age_dir_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_age_dir_train.consistency())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protected Class = Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_wrapper_education_sex(outcome, protected, unprivileged_groups, privileged_groups,\n",
    "                          favorable_label, unfavorable_label):\n",
    "    \"\"\" A wraper function to create aif360 dataset from outcome and protected in numpy array format.\n",
    "    \"\"\"\n",
    "    df2 = pd.DataFrame(data=outcome,\n",
    "                      columns=['outcome'])\n",
    "    df2['Sex'] = protected\n",
    "    \n",
    "    dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                       unfavorable_label=unfavorable_label,\n",
    "                                       df=df2,\n",
    "                                       label_names=['outcome'],\n",
    "                                       protected_attribute_names=['Sex'],\n",
    "                                       unprivileged_protected_attributes=unprivileged_groups)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "sex_to_consider = [0,1]\n",
    "unprivileged_groups = [{'Sex': 1.0}]\n",
    "privileged_groups = [{'Sex': 0.0}]\n",
    "favorable_label = 0.0\n",
    "unfavorable_label = 1.0\n",
    "\n",
    "#print(df['Sex'])\n",
    "#print(np.array(df['Sex']))\n",
    "\n",
    "myDataSet = dataset_wrapper_education_sex(outcome = np.array(df['Education']), protected = np.array(df['Sex']), \n",
    "                            unprivileged_groups=unprivileged_groups,\n",
    "                            privileged_groups=privileged_groups,\n",
    "                            favorable_label=favorable_label,\n",
    "                            unfavorable_label=unfavorable_label)  \n",
    "\n",
    "\n",
    "# print(myDataSet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_orig = myDataSet(protected_attribute_names=['Sex'],           # this dataset also contains protected\n",
    "#                                                                           # attribute for \"sex\" which we do not\n",
    "#                                                                           # consider in this evaluation\n",
    "#                              privileged_classes=[lambda x: x == 0],      # age >=25 is considered privileged\n",
    "#                             ) # ignore sex-related attributes\n",
    "\n",
    "dataset_orig_edu = myDataSet\n",
    "dataset_orig_edu_train, dataset_orig_edu_test = dataset_orig_edu.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups2 = [{'Sex': 0}]\n",
    "unprivileged_groups2 = [{'Sex': 1}]\n",
    "\n",
    "# print(dataset_orig_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original Education Sex training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.029467\n",
      "Disparate Impact = 1.043910\n",
      "Base Rate = 0.680809\n",
      "Consistency = 0.608485\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_edu_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "display(Markdown(\"#### Original Education Sex training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_orig_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_orig_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_orig_train.consistency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Reweighed Education Sex training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.001841\n",
      "Disparate Impact = 0.997279\n",
      "Base Rate = 0.676127\n",
      "Consistency = 0.631766\n"
     ]
    }
   ],
   "source": [
    "## REWEIGHING\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups2,\n",
    "                privileged_groups=privileged_groups2)\n",
    "dataset_transf_edu = RW.fit_transform(dataset_orig_edu)\n",
    "dataset_transf_edu_train, dataset_transf_edu_test = dataset_transf_edu.split([0.7], shuffle=True)\n",
    "\n",
    "\n",
    "metric_RW_train = BinaryLabelDatasetMetric(dataset_transf_edu_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "display(Markdown(\"#### Reweighed Education Sex training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_RW_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_RW_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_RW_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_RW_train.consistency())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### DIR Education Sex training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.031415\n",
      "Disparate Impact = 1.047079\n",
      "Base Rate = 0.677738\n",
      "Consistency = 0.535548\n"
     ]
    }
   ],
   "source": [
    "# Disparate Impact Remover\n",
    "\n",
    "DIR = DisparateImpactRemover(repair_level = 1.0)\n",
    "\n",
    "dataset_transf_edu_DIR = DIR.fit_transform(dataset_orig_edu)\n",
    "dataset_transf_edu_DIR_train, dataset_transf_edu_DIR_test = dataset_transf_edu_DIR.split([0.7], shuffle=True)\n",
    "\n",
    "metric_DIR_train = BinaryLabelDatasetMetric(dataset_transf_edu_DIR_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "\n",
    "# print()\n",
    "\n",
    "display(Markdown(\"#### DIR Education Sex training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_DIR_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_DIR_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_DIR_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_DIR_train.consistency())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protected Class = Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_wrapper_education_age(outcome, protected, unprivileged_groups, privileged_groups,\n",
    "                          favorable_label, unfavorable_label):\n",
    "    \"\"\" A wraper function to create aif360 dataset from outcome and protected in numpy array format.\n",
    "    \"\"\"\n",
    "    df2 = pd.DataFrame(data=outcome,\n",
    "                      columns=['outcome'])\n",
    "    df2['Age2'] = protected\n",
    "    \n",
    "    dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                       unfavorable_label=unfavorable_label,\n",
    "                                       df=df2,\n",
    "                                       label_names=['outcome'],\n",
    "                                       protected_attribute_names=['Age2'],\n",
    "                                       unprivileged_protected_attributes=unprivileged_groups)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "sex_to_consider = [0,1]\n",
    "unprivileged_groups = [{'Age2': 0.0}]\n",
    "privileged_groups = [{'Age2': 1.0}]\n",
    "favorable_label = 0.0\n",
    "unfavorable_label = 1.0\n",
    "\n",
    "#print(df['Sex'])\n",
    "#print(np.array(df['Sex']))\n",
    "\n",
    "myDataSet = dataset_wrapper_education_age(outcome = np.array(df['Education']), protected = np.array(df['Age2']), \n",
    "                            unprivileged_groups=unprivileged_groups,\n",
    "                            privileged_groups=privileged_groups,\n",
    "                            favorable_label=favorable_label,\n",
    "                            unfavorable_label=unfavorable_label)  \n",
    "\n",
    "\n",
    "# print(myDataSet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_orig = myDataSet(protected_attribute_names=['Sex'],           # this dataset also contains protected\n",
    "#                                                                           # attribute for \"sex\" which we do not\n",
    "#                                                                           # consider in this evaluation\n",
    "#                              privileged_classes=[lambda x: x == 0],      # age >=25 is considered privileged\n",
    "#                             ) # ignore sex-related attributes\n",
    "\n",
    "dataset_orig_edu_age = myDataSet\n",
    "dataset_orig_edu_age_train, dataset_orig_edu_age_test = dataset_orig_edu_age.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups2 = [{'Age2': 1}]\n",
    "unprivileged_groups2 = [{'Age2': 0}]\n",
    "\n",
    "# print(dataset_orig_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original Education Age training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.065761\n",
      "Disparate Impact = 0.906789\n",
      "Base Rate = 0.676641\n",
      "Consistency = 0.510793\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_edu_age_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "display(Markdown(\"#### Original Education Age training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_orig_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_orig_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_orig_train.consistency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Reweighed Education Age training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.006810\n",
      "Disparate Impact = 0.989945\n",
      "Base Rate = 0.674261\n",
      "Consistency = 0.580774\n"
     ]
    }
   ],
   "source": [
    "## REWEIGHING\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups2,\n",
    "                privileged_groups=privileged_groups2)\n",
    "dataset_transf_edu_age = RW.fit_transform(dataset_orig_edu_age)\n",
    "dataset_transf_edu_age_train, dataset_transf_edu_age_test = dataset_transf_edu_age.split([0.7], shuffle=True)\n",
    "\n",
    "\n",
    "metric_RW_train = BinaryLabelDatasetMetric(dataset_transf_edu_age_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "display(Markdown(\"#### Reweighed Education Age training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_RW_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_RW_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_RW_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_RW_train.consistency())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### DIR Education Age training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.062833\n",
      "Disparate Impact = 0.910945\n",
      "Base Rate = 0.678308\n",
      "Consistency = 0.489084\n"
     ]
    }
   ],
   "source": [
    "# Disparate Impact Remover\n",
    "\n",
    "DIR = DisparateImpactRemover(repair_level = 1.0)\n",
    "\n",
    "dataset_transf_edu_age_DIR = DIR.fit_transform(dataset_orig_edu_age)\n",
    "dataset_transf_edu_age_DIR_train, dataset_transf_edu_age_DIR_test = dataset_transf_edu_age_DIR.split([0.7], shuffle=True)\n",
    "\n",
    "metric_DIR_train = BinaryLabelDatasetMetric(dataset_transf_edu_age_DIR_train, \n",
    "                                             unprivileged_groups=unprivileged_groups2,\n",
    "                                             privileged_groups=privileged_groups2)\n",
    "\n",
    "# print()\n",
    "\n",
    "display(Markdown(\"#### DIR Education Age training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_DIR_train.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_DIR_train.disparate_impact())\n",
    "print(\"Base Rate = %f\" % metric_DIR_train.base_rate())\n",
    "print(\"Consistency = %f\" % metric_DIR_train.consistency())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset - Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_step4_train, dataset_orig_step4_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "dataset_orig_step4_valid, dataset_orig_step4_test = dataset_orig_step4_vt.split([0.5], shuffle=True)\n",
    "\n",
    "unprivileged_groups = [{'Sex': 1.0}]\n",
    "privileged_groups = [{'Sex': 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chris/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_step4_train.features)\n",
    "y_train = dataset_orig_step4_train.labels.ravel()\n",
    "w_train = dataset_orig_step4_train.instance_weights.ravel()\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train, \n",
    "         sample_weight=dataset_orig_step4_train.instance_weights)\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_orig_step4_train.favorable_label)[0][0]\n",
    "\n",
    "dataset_orig_step4_train_pred = dataset_orig_step4_train.copy()\n",
    "dataset_orig_step4_train_pred.labels = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_step4_valid_pred = dataset_orig_step4_valid.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_step4_valid_pred.features)\n",
    "y_valid = dataset_orig_step4_valid_pred.labels\n",
    "dataset_orig_step4_valid_pred.scores = lmod.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "dataset_orig_step4_test_pred = dataset_orig_step4_test.copy(deepcopy=True)\n",
    "X_test = scale_orig.transform(dataset_orig_step4_test_pred.features)\n",
    "y_test = dataset_orig_step4_test_pred.labels\n",
    "dataset_orig_step4_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no reweighing) = 0.6177\n",
      "Optimal classification threshold (no reweighing) = 0.7029\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_step4_valid_pred.scores > class_thresh\n",
    "    dataset_orig_step4_valid_pred.labels[fav_inds] = dataset_orig_step4_valid_pred.favorable_label\n",
    "    dataset_orig_step4_valid_pred.labels[~fav_inds] = dataset_orig_step4_valid_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_step4_valid,\n",
    "                                             dataset_orig_step4_valid_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no reweighing) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no reweighing) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Predictions from original testing data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:00<00:00, 136.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification threshold used = 0.7029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:00<00:00, 151.30it/s]/Users/Chris/miniconda3/lib/python3.6/site-packages/aif360/metrics/dataset_metric.py:93: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n",
      " 83%|████████▎ | 83/100 [00:00<00:00, 161.92it/s]/Users/Chris/miniconda3/lib/python3.6/site-packages/aif360/metrics/dataset_metric.py:93: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n",
      "100%|██████████| 100/100 [00:00<00:00, 163.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6196\n",
      "Statistical parity difference = 1.0000\n",
      "Disparate impact = inf\n",
      "Average odds difference = 1.0000\n",
      "Equal opportunity difference = 1.0000\n",
      "Theil index = 0.6525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"#### Predictions from original testing data\"))\n",
    "bal_acc_arr_orig = []\n",
    "disp_imp_arr_orig = []\n",
    "avg_odds_diff_arr_orig = []\n",
    "\n",
    "print(\"Classification threshold used = %.4f\" % best_class_thresh)\n",
    "for thresh in tqdm(class_thresh_arr):\n",
    "    \n",
    "    if thresh == best_class_thresh:\n",
    "        disp = True\n",
    "    else:\n",
    "        disp = False\n",
    "    \n",
    "    fav_inds = dataset_orig_step4_test_pred.scores > thresh\n",
    "    dataset_orig_step4_test_pred.labels[fav_inds] = dataset_orig_step4_test_pred.favorable_label\n",
    "    dataset_orig_step4_test_pred.labels[~fav_inds] = dataset_orig_step4_test_pred.unfavorable_label\n",
    "    \n",
    "    metric_test_bef = compute_metrics(dataset_orig_step4_test, dataset_orig_step4_test_pred, \n",
    "                                      unprivileged_groups, privileged_groups,\n",
    "                                      disp = disp)\n",
    "\n",
    "    bal_acc_arr_orig.append(metric_test_bef[\"Balanced accuracy\"])\n",
    "    avg_odds_diff_arr_orig.append(metric_test_bef[\"Average odds difference\"])\n",
    "    disp_imp_arr_orig.append(metric_test_bef[\"Disparate impact\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reweighing Dataset - Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chris/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scale_transf = StandardScaler()\n",
    "X_train = scale_transf.fit_transform(dataset_transf_train.features)\n",
    "y_train = dataset_transf_train.labels.ravel()\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train,\n",
    "        sample_weight=dataset_transf_train.instance_weights)\n",
    "y_train_pred = lmod.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transf_test_pred = dataset_transf_test.copy(deepcopy=True)\n",
    "X_test = scale_transf.fit_transform(dataset_transf_test_pred.features)\n",
    "y_test = dataset_transf_test_pred.labels\n",
    "dataset_transf_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Predictions from transformed testing data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:00<00:01, 85.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification threshold used = 0.7029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:00<00:00, 103.01it/s]/Users/Chris/miniconda3/lib/python3.6/site-packages/aif360/metrics/dataset_metric.py:93: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n",
      " 84%|████████▍ | 84/100 [00:00<00:00, 106.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5000\n",
      "Statistical parity difference = 0.0000\n",
      "Disparate impact = 1.0000\n",
      "Average odds difference = 0.0000\n",
      "Equal opportunity difference = 0.0000\n",
      "Theil index = 0.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 99.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# print(dataset_transf_test)\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Predictions from transformed testing data\"))\n",
    "bal_acc_arr_transf = []\n",
    "disp_imp_arr_transf = []\n",
    "avg_odds_diff_arr_transf = []\n",
    "\n",
    "print(\"Classification threshold used = %.4f\" % best_class_thresh)\n",
    "for thresh in tqdm(class_thresh_arr):\n",
    "    \n",
    "    if thresh == best_class_thresh:\n",
    "        disp = True\n",
    "    else:\n",
    "        disp = False\n",
    "    \n",
    "    fav_inds = dataset_transf_test_pred.scores > thresh\n",
    "    dataset_transf_test_pred.labels[fav_inds] = dataset_transf_test_pred.favorable_label\n",
    "    dataset_transf_test_pred.labels[~fav_inds] = dataset_transf_test_pred.unfavorable_label\n",
    "    \n",
    "#     print(dataset_transf_test_pred)\n",
    "    \n",
    "    metric_test_aft = compute_metrics(dataset_transf_test, dataset_transf_test_pred, \n",
    "                                      unprivileged_groups, privileged_groups,\n",
    "                                      disp = disp)\n",
    "\n",
    "    bal_acc_arr_transf.append(metric_test_aft[\"Balanced accuracy\"])\n",
    "    avg_odds_diff_arr_transf.append(metric_test_aft[\"Average odds difference\"])\n",
    "    disp_imp_arr_transf.append(metric_test_aft[\"Disparate impact\"])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
